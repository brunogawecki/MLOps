{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cef1206a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f409f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21c07c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X.iloc[idx].values.reshape(28, 28).astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        label = torch.tensor(self.y.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815881f",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a95e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='data/fashion_mnist', batch_size=64, val_split=0.2, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv('data/fashion_mnist/fashion-mnist_train.csv')\n",
    "        test_df = pd.read_csv('data/fashion_mnist/fashion-mnist_test.csv')\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=self.val_split, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "        X_train = train_df.drop(columns=['label'])\n",
    "        y_train = train_df['label']\n",
    "\n",
    "        X_val = val_df.drop(columns=['label'])\n",
    "        y_val = val_df['label']\n",
    "\n",
    "        X_test = test_df.drop(columns=['label'])\n",
    "        y_test = test_df['label']\n",
    "\n",
    "        # Create datasets based on stage\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = FashionMNISTDataset(X_train, y_train, transform=self.transform)\n",
    "            self.val_dataset = FashionMNISTDataset(X_val, y_val, transform=self.transform)\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = FashionMNISTDataset(X_test, y_test, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b9c28",
   "metadata": {},
   "source": [
    "# Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26f69984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTLightningModule(pl.LightningModule):\n",
    "    def __init__(self, dropout_rate=0.25, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = CNNModel(dropout_rate=dropout_rate)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension -> (batch, 1, height, width)\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20adea0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f008af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 7 * 7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e74b4",
   "metadata": {},
   "source": [
    "# Baseline Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create model\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Creating model...\")\n",
    "print(\"=\" * 60)\n",
    "model = FashionMNISTLightningModule(dropout_rate=0.25, learning_rate=0.001)\n",
    "print(f\"✓ Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# 2. Set up data module\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Setting up data module...\")\n",
    "print(\"=\" * 60)\n",
    "data_module = FashionMNISTDataModule()\n",
    "# Note: setup() will be called automatically by PyTorch Lightning\n",
    "# But we can call it manually to see the data info\n",
    "data_module.setup()\n",
    "print(f\"✓ Training samples: {len(data_module.train_dataset):,}\")\n",
    "print(f\"✓ Validation samples: {len(data_module.val_dataset):,}\")\n",
    "print(f\"✓ Test samples: {len(data_module.test_dataset):,}\")\n",
    "\n",
    "# 3. Set up WandB logger\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Initializing WandB...\")\n",
    "print(\"=\" * 60)\n",
    "wandb_logger = WandbLogger(\n",
    "    project='Fashion_MNIST', \n",
    "    name='baseline-cnn',\n",
    "    log_model=True  # Log model checkpoints to WandB\n",
    ")\n",
    "print(\"✓ WandB logger initialized\")\n",
    "\n",
    "# 4. Set up callbacks\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Setting up callbacks...\")\n",
    "print(\"=\" * 60)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='fashion-mnist-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    "    verbose=True  # Print when checkpoints are saved\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "print(\"✓ ModelCheckpoint callback configured\")\n",
    "print(\"✓ EarlyStopping callback configured\")\n",
    "\n",
    "# 5. Create trainer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Creating trainer...\")\n",
    "print(\"=\" * 60)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    enable_progress_bar=True,  # Show progress bar\n",
    "    enable_model_summary=True,  # Show model summary\n",
    "    log_every_n_steps=50,  # Log every 50 steps\n",
    "    val_check_interval=1.0,  # Validate after each epoch\n",
    "    check_val_every_n_epoch=1  # Check validation every epoch\n",
    ")\n",
    "print(\"✓ Trainer created\")\n",
    "print(f\"✓ Max epochs: {trainer.max_epochs}\")\n",
    "print(f\"✓ Batch size: {data_module.batch_size}\")\n",
    "\n",
    "# 6. Train the model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"You will see progress bars showing:\")\n",
    "print(\"  - Current epoch / Total epochs\")\n",
    "print(\"  - Training loss and accuracy\")\n",
    "print(\"  - Validation loss and accuracy\")\n",
    "print(\"  - Progress through batches\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Training completed!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show best model info\n",
    "if checkpoint_callback.best_model_path:\n",
    "    print(f\"\\nBest model saved at: {checkpoint_callback.best_model_path}\")\n",
    "    print(f\"Best validation loss: {checkpoint_callback.best_model_score:.4f}\")\n",
    "\n",
    "# 7. Test the model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Evaluating on test set...\")\n",
    "print(\"=\" * 60)\n",
    "test_results = trainer.test(model, data_module)\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Testing completed!\")\n",
    "\n",
    "# 8. Finish WandB run\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 8: Finalizing WandB...\")\n",
    "print(\"=\" * 60)\n",
    "wandb.finish()\n",
    "print(\"✓ WandB run finished!\")\n",
    "print(\"\\nView your results at: https://wandb.ai\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916fe23",
   "metadata": {},
   "source": [
    "# Optuna\n",
    "- Optuna callback logs trial summaries to WandB during hyperparameter optimisation.\n",
    "- WandB logger logs train/val loss curves during final training of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d815ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)  # log as in logarithmic scale\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    model = FashionMNISTLightningModule(\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    data_module = FashionMNISTDataModule(batch_size=batch_size)\n",
    "    data_module.setup()\n",
    "    \n",
    "    # No WandB logger here - Optuna callback will handle trial logging\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5,  # Fewer epochs for faster hyperparameter search\n",
    "        logger=False,  # Disable detailed logging - Optuna callback will log trial summaries\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=False,\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, mode='min')]\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    return trainer.callback_metrics['val_loss'].item()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Optuna Hyperparameter Optimization...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Tuning: dropout_rate, learning_rate, batch_size\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',  # Minimize validation loss\n",
    "    study_name='fashion_mnist_hyperopt',\n",
    "    pruner=optuna.pruners.MedianPruner()\n",
    ")\n",
    "\n",
    "wandb_kwargs = {\n",
    "    \"project\": \"Fashion_MNIST\",\n",
    "    \"group\": \"hyperparameter-optimization\",\n",
    "    \"job_type\": \"optuna-search\"\n",
    "}\n",
    "\n",
    "wandb_callback = WeightsAndBiasesCallback(\n",
    "    metric_name=\"val_loss\",\n",
    "    wandb_kwargs=wandb_kwargs,\n",
    "    as_multirun=True  # Creates separate WandB run for each trial\n",
    ")\n",
    "\n",
    "print(\"\\nRunning optimization with WandB logging...\")\n",
    "study.optimize(objective, n_trials=20, callbacks=[wandb_callback])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Best Hyperparameters Found:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nBest validation loss: {study.best_value:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "934c1eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.2, 'learning_rate': 0.0027002487286870982, 'batch_size': 64}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff24f969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optuna.study.study.Study at 0x230d77e7350>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f2632",
   "metadata": {},
   "source": [
    "# Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training final model with best hyperparameters:\n",
      "============================================================\n",
      "  dropout_rate: 0.2\n",
      "  learning_rate: 0.0027002487286870982\n",
      "  batch_size: 64\n",
      "============================================================\n",
      "\n",
      "✓ Training samples: 48,000\n",
      "✓ Validation samples: 12,000\n",
      "✓ Test samples: 10,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training final optimized model with full WandB logging...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20251129_122814-xdsaussr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST/runs/xdsaussr' target=\"_blank\">optimized-model-v2</a></strong> to <a href='https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST' target=\"_blank\">https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST/runs/xdsaussr' target=\"_blank\">https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST/runs/xdsaussr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bruno\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory C:\\Users\\Bruno\\Desktop\\MLOps\\homework1\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | CNNModel         | 105 K  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "105 K     Trainable params\n",
      "0         Non-trainable params\n",
      "105 K     Total params\n",
      "0.423     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bruno\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\Bruno\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 750/750 [00:40<00:00, 18.63it/s, v_num=ussr, train_loss_step=0.597, train_acc_step=0.812, val_loss=0.358, val_acc=0.868, train_loss_epoch=0.517, train_acc_epoch=0.812]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.358\n",
      "Epoch 0, global step 750: 'val_loss' reached 0.35781 (best 0.35781), saving model to 'C:\\\\Users\\\\Bruno\\\\Desktop\\\\MLOps\\\\homework1\\\\checkpoints\\\\fashion-mnist-optimized-epoch=00-val_loss=0.36.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 750/750 [00:38<00:00, 19.50it/s, v_num=ussr, train_loss_step=0.299, train_acc_step=0.844, val_loss=0.309, val_acc=0.890, train_loss_epoch=0.349, train_acc_epoch=0.874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.049 >= min_delta = 0.0. New best score: 0.309\n",
      "Epoch 1, global step 1500: 'val_loss' reached 0.30857 (best 0.30857), saving model to 'C:\\\\Users\\\\Bruno\\\\Desktop\\\\MLOps\\\\homework1\\\\checkpoints\\\\fashion-mnist-optimized-epoch=01-val_loss=0.31.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [00:49<00:00, 15.22it/s, v_num=ussr, train_loss_step=0.151, train_acc_step=0.938, val_loss=0.313, val_acc=0.886, train_loss_epoch=0.303, train_acc_epoch=0.891]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2250: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 750/750 [00:43<00:00, 17.33it/s, v_num=ussr, train_loss_step=0.461, train_acc_step=0.859, val_loss=0.267, val_acc=0.902, train_loss_epoch=0.274, train_acc_epoch=0.899] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.267\n",
      "Epoch 3, global step 3000: 'val_loss' reached 0.26682 (best 0.26682), saving model to 'C:\\\\Users\\\\Bruno\\\\Desktop\\\\MLOps\\\\homework1\\\\checkpoints\\\\fashion-mnist-optimized-epoch=03-val_loss=0.27.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:40<00:00, 18.34it/s, v_num=ussr, train_loss_step=0.235, train_acc_step=0.922, val_loss=0.251, val_acc=0.907, train_loss_epoch=0.251, train_acc_epoch=0.908] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.251\n",
      "Epoch 4, global step 3750: 'val_loss' reached 0.25123 (best 0.25123), saving model to 'C:\\\\Users\\\\Bruno\\\\Desktop\\\\MLOps\\\\homework1\\\\checkpoints\\\\fashion-mnist-optimized-epoch=04-val_loss=0.25-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 750/750 [00:44<00:00, 16.91it/s, v_num=ussr, train_loss_step=0.265, train_acc_step=0.875, val_loss=0.249, val_acc=0.912, train_loss_epoch=0.233, train_acc_epoch=0.913] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.249\n",
      "Epoch 5, global step 4500: 'val_loss' reached 0.24907 (best 0.24907), saving model to 'C:\\\\Users\\\\Bruno\\\\Desktop\\\\MLOps\\\\homework1\\\\checkpoints\\\\fashion-mnist-optimized-epoch=05-val_loss=0.25.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 750/750 [00:39<00:00, 18.80it/s, v_num=ussr, train_loss_step=0.375, train_acc_step=0.891, val_loss=0.262, val_acc=0.906, train_loss_epoch=0.219, train_acc_epoch=0.920] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 5250: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 750/750 [00:40<00:00, 18.42it/s, v_num=ussr, train_loss_step=0.243, train_acc_step=0.922, val_loss=0.258, val_acc=0.907, train_loss_epoch=0.208, train_acc_epoch=0.923] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 6000: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 750/750 [00:40<00:00, 18.34it/s, v_num=ussr, train_loss_step=0.142, train_acc_step=0.953, val_loss=0.250, val_acc=0.911, train_loss_epoch=0.191, train_acc_epoch=0.928] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 6750: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 750/750 [00:41<00:00, 18.21it/s, v_num=ussr, train_loss_step=0.255, train_acc_step=0.906, val_loss=0.251, val_acc=0.913, train_loss_epoch=0.179, train_acc_epoch=0.932] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 7500: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 750/750 [00:41<00:00, 18.20it/s, v_num=ussr, train_loss_step=0.255, train_acc_step=0.906, val_loss=0.251, val_acc=0.913, train_loss_epoch=0.179, train_acc_epoch=0.932]\n",
      "\n",
      "============================================================\n",
      "✓ Training completed!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating optimized model on test set...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bruno\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:04<00:00, 37.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9189000129699707     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.23339980840682983    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9189000129699707    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.23339980840682983   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✓ Testing completed!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆█████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁▅▆▆▇▇▇▇██</td></tr><tr><td>train_acc_step</td><td>▁▇▃▃▁▃▅▃▂▂▅▇▅▅▅▂▅▇▅▂▇▅▅▇▄▅▅▆█▇▅▇▆▇█▆▇▇▇▅</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>▆▇▇▆▄█▄▆▇▅▆▄▅█▃▂▆▅▅▄▃▃▁▃▃▂▂▂▃▃▄▂▂▂▁▃▂▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>val_acc</td><td>▁▄▄▆▇█▇▇██</td></tr><tr><td>val_loss</td><td>█▅▅▂▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_acc</td><td>0.9189</td></tr><tr><td>test_loss</td><td>0.2334</td></tr><tr><td>train_acc_epoch</td><td>0.93238</td></tr><tr><td>train_acc_step</td><td>0.90625</td></tr><tr><td>train_loss_epoch</td><td>0.17933</td></tr><tr><td>train_loss_step</td><td>0.25466</td></tr><tr><td>trainer/global_step</td><td>7500</td></tr><tr><td>val_acc</td><td>0.91342</td></tr><tr><td>val_loss</td><td>0.25134</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">optimized-model-v2</strong> at: <a href='https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST/runs/xdsaussr' target=\"_blank\">https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST/runs/xdsaussr</a><br> View project at: <a href='https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST' target=\"_blank\">https://wandb.ai/brunogawecki0-poznan-university-of-technology/Fashion_MNIST</a><br>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251129_122814-xdsaussr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ WandB run finished. Check your dashboard at https://wandb.ai\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "print(\"=\" * 60)\n",
    "print(\"Training final model with best hyperparameters:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create final model with best hyperparameters\n",
    "final_model = FashionMNISTLightningModule(\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "# Create data module with best batch size\n",
    "final_data_module = FashionMNISTDataModule(batch_size=best_params['batch_size'])\n",
    "final_data_module.setup()\n",
    "\n",
    "print(f\"\\n✓ Training samples: {len(final_data_module.train_dataset):,}\")\n",
    "print(f\"✓ Validation samples: {len(final_data_module.val_dataset):,}\")\n",
    "print(f\"✓ Test samples: {len(final_data_module.test_dataset):,}\")\n",
    "\n",
    "# Set up WandB logger for final training (full curve logging)\n",
    "wandb_logger_final = WandbLogger(\n",
    "    project='Fashion_MNIST', \n",
    "    name='optimized-model-v2',\n",
    "    group='final-training',\n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_callback_final = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='fashion-mnist-optimized-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback_final = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create trainer with full epochs and WandB logging\n",
    "trainer_final = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=wandb_logger_final,  # Full WandB logging for training/validation curves\n",
    "    callbacks=[checkpoint_callback_final, early_stop_callback_final],\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    log_every_n_steps=50\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training final optimized model with full WandB logging...\")\n",
    "print(\"=\" * 60)\n",
    "trainer_final.fit(final_model, final_data_module)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Training completed!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test final model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating optimized model on test set...\")\n",
    "print(\"=\" * 60)\n",
    "trainer_final.test(final_model, final_data_module)\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Testing completed!\")\n",
    "\n",
    "# Finish WandB\n",
    "wandb.finish()\n",
    "print(\"\\n✓ WandB run finished. Check your dashboard at https://wandb.ai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ef0de",
   "metadata": {},
   "source": [
    "# Plots\n",
    "### Baseline model:\n",
    "Robust steady and slow decrese in loss.\n",
    "- Test loss: 0.2314\n",
    "- Test accuracy: 91.3%\n",
    "\n",
    "<img src=\"images/baseline_model-train_val_loss.png\" width=60%>\n",
    "\n",
    "### Hyperparameter finetuning:\n",
    "```\n",
    "dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "```\n",
    "\n",
    "<img src=\"images/tuning-val_loss.png\" width=60%>\n",
    "\n",
    "### Tuned model:\n",
    "Faster initial drop in loss, but overfitting in later stages.\n",
    "- Test loss: 0.2334\n",
    "- Test accuracy: 91.9%\n",
    "\n",
    "<img src=\"images/optimized_model-train_val_loss.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509a21c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
